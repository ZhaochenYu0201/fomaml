# META-LORAå¿«é€Ÿè¿è¡ŒæŒ‡å—

## ğŸ¯ æ ¸å¿ƒä»·å€¼ä¸»å¼ 

**META-LORA = FOMAMLçš„æ€§èƒ½ + 10-100å€çš„é€Ÿåº¦æå‡**

- âœ… åªç”¨100æ ·æœ¬/ä»»åŠ¡ï¼ˆvs FOMAMLçš„300æ ·æœ¬ï¼‰
- âœ… Base modelå†»ç»“ï¼ˆæä½å†…å­˜å ç”¨ï¼‰
- âœ… åªä¼˜åŒ–LoRAå‚æ•°ï¼ˆè®­ç»ƒé€Ÿåº¦å¿«10-100å€ï¼‰
- âœ… Checkpointè¶…å°ï¼ˆåªæœ‰LoRAå‚æ•°ï¼Œ~1-10MBï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### Step 1: å‡†å¤‡æ•°æ®ï¼ˆä¸FOMAMLç›¸åŒï¼‰

```bash
# ä½¿ç”¨ç°æœ‰çš„æ•°æ®å‡†å¤‡è„šæœ¬
python prepare_math_science_data.py \
    --output-dir ./data/math_science_meta \
    --support-ratio 0.15 \  # è°ƒæ•´ä»¥è·å¾—~100æ ·æœ¬/ä»»åŠ¡
    --query-ratio 0.25
```

### Step 2: è®­ç»ƒMETA-LORA

```bash
# å•GPUï¼ˆå¯è¡Œï¼META-LORAå†…å­˜å ç”¨ä½ï¼‰
python meta_lora_trainer.py \
    --config-name config_meta_lora_example

# å¤šGPUï¼ˆæ›´å¿«ï¼‰
torchrun --nproc_per_node=4 \
    meta_lora_trainer.py \
    --config-name config_meta_lora_example

# é¢„æœŸè®­ç»ƒæ—¶é—´:
# - å•å¡A100: ~8-10å°æ—¶
# - 4å¡A100: ~2-3å°æ—¶
# vs FOMAML: 40-60å°æ—¶ï¼
```

### Step 3: è¯„ä¼°Few-Shotæ€§èƒ½

```bash
python evaluate_few_shot.py \
    --model-path ./checkpoints/meta_lora/meta_lora_checkpoint_step_3000.pt \
    --model-type meta_lora \
    --eval-tasks calculus theorem_proving \
    --n-shots 0 5 10 25 50 \
    --output-dir ./results/meta_lora
```

---

## ğŸ“Š ä¸FOMAMLå¯¹æ¯”

| ç‰¹æ€§ | FOMAML-SFT | META-LORA | å·®å¼‚ |
|------|------------|-----------|------|
| **è®­ç»ƒæ—¶é—´** | 40-60h | 4-6h | **10x faster** |
| **GPUå†…å­˜** | 70GB | 30GB | **2.3x less** |
| **æ¯æ­¥è€—æ—¶** | 30-60s | 3-6s | **10x faster** |
| **Checkpointå¤§å°** | 2-5GB | 5-20MB | **100-500x smaller** |
| **æ¯ä»»åŠ¡æ ·æœ¬æ•°** | 300 | 100 | **3x less data** |
| **Few-shotæ€§èƒ½** | ä¼˜ç§€ | ä¼˜ç§€ï¼ˆé¢„æœŸç›¸è¿‘ï¼‰ | â‰ˆ |

---

## ğŸ”¬ æ ¸å¿ƒå®ç°ç»†èŠ‚

### ç®—æ³•ä¼ªä»£ç 

```python
# META-LORA Two-Stage Optimization

# åˆå§‹åŒ–
base_model = load_model()  # å®Œå…¨å†»ç»“
base_model.freeze()
shared_lora = initialize_lora(rank=16)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    # é‡‡æ ·ä»»åŠ¡æ‰¹æ¬¡
    task_batch = sample_tasks(n=4)

    # ===== Stage 1: Task-Specific Adaptation =====
    adapted_loras = {}
    for task in task_batch:
        # ä»shared LoRAå¼€å§‹
        task_lora = clone(shared_lora)

        # åœ¨100ä¸ªæ ·æœ¬ä¸Šå¿«é€Ÿé€‚åº”
        for k in range(10):  # 10 steps
            batch = sample(task.train_data, n=4)
            loss = compute_loss(base_model + task_lora, batch)
            task_lora = task_lora - inner_lr * grad(loss)

        adapted_loras[task] = task_lora

    # ===== Stage 2: Shared LoRA Update =====
    meta_grad = 0
    for task in task_batch:
        # åŠ è½½adapted LoRA
        load_lora(adapted_loras[task])

        # åœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ¢¯åº¦
        val_batch = sample(task.val_data)
        val_loss = compute_loss(base_model + adapted_loras[task], val_batch)

        # æ¢¯åº¦èšåˆ
        meta_grad += grad(val_loss)

    # æ›´æ–°shared LoRA
    shared_lora = shared_lora - meta_lr * (meta_grad / len(task_batch))
```

### å…³é”®ä»£ç ç‰‡æ®µ

```python
# 1. å†»ç»“base model
for param in base_model.parameters():
    param.requires_grad = False

# 2. æ·»åŠ LoRA
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # rank
    lora_alpha=32,  # é€šå¸¸ alpha = 2*r
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
)
model = get_peft_model(base_model, lora_config)

# 3. åªä¼˜åŒ–LoRAå‚æ•°
lora_params = [p for p in model.parameters() if p.requires_grad]
optimizer = AdamW(lora_params, lr=5e-5)
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### å…³é”®è¶…å‚æ•°

```yaml
meta:
  # Stage 1: ä»»åŠ¡é€‚åº”
  inner_lr: 1e-4          # é€‚åº”å­¦ä¹ ç‡ï¼ˆè¾ƒå¤§ï¼‰
  num_inner_steps: 10     # é€‚åº”æ­¥æ•°ï¼ˆ5-20åˆé€‚ï¼‰
  inner_batch_size: 4

  # Stage 2: å…ƒæ›´æ–°
  meta_lr: 5e-5          # å…ƒå­¦ä¹ ç‡ï¼ˆçº¦ä¸ºinner_lrçš„1/2ï¼‰
  meta_batch_size: 4     # æ¯æ¬¡meta-updateçš„ä»»åŠ¡æ•°

model:
  lora_rank: 16          # LoRAç§©ï¼ˆè¶Šå¤§å®¹é‡è¶Šå¤§ä½†è¶Šæ…¢ï¼‰
  lora_alpha: 32         # ç¼©æ”¾å› å­ï¼ˆé€šå¸¸ = 2*rankï¼‰
  lora_dropout: 0.05     # Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
```

### è¶…å‚æ•°è°ƒä¼˜å»ºè®®

**Inner LR (å†…å¾ªç¯å­¦ä¹ ç‡)**
```python
# å¤ªå°: é€‚åº”ä¸å……åˆ†
# å¤ªå¤§: ä¸ç¨³å®š
# æ¨èèŒƒå›´: [5e-5, 2e-4]
# ä» 1e-4 å¼€å§‹
```

**Meta LR (å¤–å¾ªç¯å­¦ä¹ ç‡)**
```python
# é€šå¸¸æ¯” inner_lr å° 2-5å€
# æ¨èèŒƒå›´: [1e-5, 1e-4]
# ä» inner_lr / 2 å¼€å§‹
```

**LoRA Rank**
```python
# r=4: å¤ªå°ï¼Œå®¹é‡ä¸è¶³
# r=8: æœ€å°å¯è¡Œå€¼
# r=16: æ¨èé»˜è®¤å€¼ â­
# r=32: æ›´å¤§å®¹é‡ï¼Œä½†è®­ç»ƒæ…¢
# r=64: å¾ˆå°‘éœ€è¦è¿™ä¹ˆå¤§
```

**Inner Steps**
```python
# K=3: å¿«é€ŸåŸå‹
# K=5-10: æ¨èèŒƒå›´ â­
# K=20: å¯èƒ½è¿‡æ‹Ÿåˆsupport set
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### å…³é”®æŒ‡æ ‡

```python
# 1. Stage 1 é€‚åº”loss
# åº”è¯¥åœ¨10æ­¥å†…å¿«é€Ÿä¸‹é™
stage1/task_i/loss: 2.5 â†’ 0.8  âœ…

# 2. Stage 2 éªŒè¯loss
# åº”è¯¥æŒç»­ä¸‹é™
stage2/meta_loss: é€æ¸é™ä½  âœ…

# 3. é€‚åº”é—´éš™
# val_loss - train_loss
# åº”è¯¥å‡å°ï¼Œè¯´æ˜æ³›åŒ–æå‡
adaptation_gap: é€æ¸ç¼©å°  âœ…

# 4. GPUå†…å­˜
# åº”è¯¥æ˜¾è‘—ä½äºFOMAML
gpu_memory: ~30GB (vs FOMAML ~70GB)  âœ…

# 5. æ¯æ­¥æ—¶é—´
# åº”è¯¥æ˜¾è‘—å¿«äºFOMAML
time_per_step: ~3-6s (vs FOMAML ~30-60s)  âœ…
```

### Wandbå¯è§†åŒ–

```python
# æŸ¥çœ‹è¿™äº›å›¾è¡¨ï¼š
1. "stage1/{task}/loss" - å„ä»»åŠ¡çš„é€‚åº”æ›²çº¿
2. "stage2/meta_loss" - å…ƒæŸå¤±æ›²çº¿
3. "adaptation_gap" - æ³›åŒ–èƒ½åŠ›
4. "system/gpu_memory" - å†…å­˜å ç”¨
5. "system/time_per_step" - è®­ç»ƒé€Ÿåº¦
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: OOM (Out of Memory)

```yaml
# è§£å†³æ–¹æ¡ˆ1: å‡å°batch size
inner_batch_size: 2  # ä»4é™åˆ°2

# è§£å†³æ–¹æ¡ˆ2: å‡å°LoRA rank
lora_rank: 8  # ä»16é™åˆ°8

# è§£å†³æ–¹æ¡ˆ3: å‡å°‘inner steps
num_inner_steps: 5  # ä»10é™åˆ°5

# è§£å†³æ–¹æ¡ˆ4: ä½¿ç”¨æ›´å°çš„base model
# Llama-3.2-1B â†’ Qwen2.5-0.5B
```

### Q2: è®­ç»ƒä¸ç¨³å®š (Losséœ‡è¡)

```yaml
# è§£å†³æ–¹æ¡ˆ1: é™ä½learning rates
inner_lr: 5e-5  # ä»1e-4é™åˆ°5e-5
meta_lr: 2e-5   # ä»5e-5é™åˆ°2e-5

# è§£å†³æ–¹æ¡ˆ2: å¢åŠ æ¢¯åº¦è£å‰ª
clip_grad: 0.5  # ä»1.0é™åˆ°0.5

# è§£å†³æ–¹æ¡ˆ3: å‡å°meta_batch_size
meta_batch_size: 2  # ä»4é™åˆ°2
```

### Q3: Few-shotæ€§èƒ½ä¸å¦‚FOMAML

```yaml
# è¯Šæ–­æ­¥éª¤:
1. æ£€æŸ¥ adaptation_gap æ˜¯å¦åœ¨ä¸‹é™
   - å¦‚æœä¸ä¸‹é™ï¼šå…ƒå­¦ä¹ æ²¡æœ‰ç”Ÿæ•ˆ
   - å¦‚æœä¸‹é™ï¼šåªæ˜¯éœ€è¦æ›´é•¿è®­ç»ƒ

2. å°è¯•å¢åŠ LoRA rank
   lora_rank: 32  # å¢åŠ å®¹é‡

3. å°è¯•å¢åŠ æ¯ä»»åŠ¡æ ·æœ¬æ•°
   train_max_samples: 200  # ä»100å¢åˆ°200

4. å°è¯•å¢åŠ inner steps
   num_inner_steps: 15  # ä»10å¢åˆ°15
```

### Q4: Base modelåŠ è½½å¤±è´¥

```python
# ç¡®ä¿å®‰è£…äº†peftåº“
pip install peft

# ç¡®ä¿base modelä¸LoRAå…¼å®¹
# æ£€æŸ¥ target_modules æ˜¯å¦æ­£ç¡®
```

---

## ğŸ”¬ æ¶ˆèå®éªŒç¤ºä¾‹

### å®éªŒ1: LoRA Rankçš„å½±å“

```bash
# Rank=8
python meta_lora_trainer.py \
    model.lora_rank=8 \
    trainer.experiment_name=meta_lora_r8

# Rank=16 (default)
python meta_lora_trainer.py \
    model.lora_rank=16 \
    trainer.experiment_name=meta_lora_r16

# Rank=32
python meta_lora_trainer.py \
    model.lora_rank=32 \
    trainer.experiment_name=meta_lora_r32
```

### å®éªŒ2: æ ·æœ¬æ•°é‡çš„å½±å“

```bash
# 50 samples
python meta_lora_trainer.py \
    meta.tasks[0].train_max_samples=50 \
    trainer.experiment_name=meta_lora_50samples

# 100 samples (META-LORAè®ºæ–‡)
python meta_lora_trainer.py \
    meta.tasks[0].train_max_samples=100 \
    trainer.experiment_name=meta_lora_100samples

# 300 samples (FOMAMLè®¾å®š)
python meta_lora_trainer.py \
    meta.tasks[0].train_max_samples=300 \
    trainer.experiment_name=meta_lora_300samples
```

---

## ğŸ“š å¯¹æ¯”FOMAMLä½¿ç”¨

### ä½•æ—¶ä½¿ç”¨META-LORAï¼Ÿ

âœ… **META-LORAé€‚åˆï¼š**
- è®¡ç®—èµ„æºæœ‰é™
- éœ€è¦å¿«é€Ÿå®éªŒè¿­ä»£
- ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼ˆLoRAå®¹é‡è¶³å¤Ÿï¼‰
- æƒ³è¦å°checkpointï¼ˆä¾¿äºåˆ†äº«ï¼‰
- åªæœ‰å°‘é‡æ•°æ®ï¼ˆ100æ ·æœ¬/ä»»åŠ¡ï¼‰

âœ… **FOMAMLé€‚åˆï¼š**
- ä»»åŠ¡æå…¶å¤æ‚
- æœ‰å……è¶³è®¡ç®—èµ„æº
- éœ€è¦æœ€ä½³æ€§èƒ½ï¼ˆä¸è®¡æˆæœ¬ï¼‰
- æœ‰å……è¶³æ•°æ®ï¼ˆ300+æ ·æœ¬/ä»»åŠ¡ï¼‰

### å¹¶è¡Œä½¿ç”¨ç­–ç•¥

```
é˜¶æ®µ1: å¿«é€ŸåŸå‹ï¼ˆMETA-LORAï¼‰
- éªŒè¯æƒ³æ³•å¯è¡Œæ€§
- æ¢ç´¢è¶…å‚æ•°ç©ºé—´
- 1-2å¤©å®Œæˆåˆæ­¥å®éªŒ

é˜¶æ®µ2: ç²¾ç»†ä¼˜åŒ–ï¼ˆFOMAMLæˆ–META-LORAï¼‰
- å¦‚æœMETA-LORAæ€§èƒ½è¶³å¤Ÿå¥½ï¼šç»§ç»­ç”¨
- å¦‚æœéœ€è¦squeezeå‡ºæœ€åçš„æ€§èƒ½ï¼šç”¨FOMAML
- 3-5å¤©å®Œæˆæœ€ç»ˆè®­ç»ƒ
```

---

## ğŸ“ ç†è§£META-LORA

### ä¸ºä»€ä¹ˆåªä¼˜åŒ–LoRAå‚æ•°å°±å¤Ÿäº†ï¼Ÿ

```
ç›´è§‰è§£é‡Š:
1. Base modelå·²ç»ç¼–ç äº†é€šç”¨è¯­è¨€ç†è§£
2. LoRAå­¦ä¹ çš„æ˜¯"ä»»åŠ¡ç‰¹å®šçš„è°ƒæ•´"
3. Meta-learningå­¦ä¹ çš„æ˜¯"å¦‚ä½•å¿«é€Ÿè°ƒæ•´"

æŠ€æœ¯è§£é‡Š:
- LoRAåœ¨ä½ç§©å­ç©ºé—´ä¸­æ“ä½œ
- å¯¹äºå¤šä»»åŠ¡å­¦ä¹ ï¼Œå…±äº«çš„ä½ç§©ç»“æ„åŒ…å«äº†ä»»åŠ¡å…±æ€§
- å…ƒå­¦ä¹ ä¼˜åŒ–è¿™ä¸ªå…±äº«ç»“æ„ï¼Œä½¿å…¶æ˜“äºé€‚åº”
```

### ä¸MAMLçš„ç†è®ºè”ç³»

```
MAML: å­¦ä¹ å‚æ•°åˆå§‹åŒ–Î¸*
      ä½¿å¾— Î¸* - Î±âˆ‡L_i(Î¸*) åœ¨ä»»åŠ¡iä¸Šæ€§èƒ½å¥½

META-LORA: å­¦ä¹ LoRAåˆå§‹åŒ–Ïˆ*
           ä½¿å¾— Ïˆ* - Î±âˆ‡L_i(Ïˆ*) åœ¨ä»»åŠ¡iä¸Šæ€§èƒ½å¥½
           base model Î¸_base å›ºå®š

å…³é”®: Ïˆ*çš„å‚æ•°ç©ºé—´æ¯”Î¸*å°å¾—å¤šï¼ˆ0.1-1% vs 100%ï¼‰
     ä½†å¯¹äºç›¸å…³ä»»åŠ¡ï¼Œä½ç§©è°ƒæ•´è¶³å¤Ÿ
```

---

## ğŸ“– å‚è€ƒèµ„æº

### è®ºæ–‡
- **META-LORA**: arXiv:2510.11598 (ICLR 2026)
- **MAML**: Finn et al., ICML 2017
- **LoRA**: Hu et al., ICLR 2022

### ä»£ç 
- æœ¬å®ç°: `meta_lora_trainer.py`
- PEFTåº“: https://github.com/huggingface/peft
- verlæ¡†æ¶: https://github.com/volcengine/verl

---

## âœ… å®éªŒæ£€æŸ¥æ¸…å•

å®Œæ•´META-LORAå®éªŒåº”è¯¥åŒ…æ‹¬ï¼š

- [ ] æ•°æ®å‡†å¤‡ï¼ˆ100æ ·æœ¬/ä»»åŠ¡ï¼‰
- [ ] META-LORAè®­ç»ƒ
- [ ] FOMAMLè®­ç»ƒï¼ˆå¯¹æ¯”åŸºå‡†ï¼‰
- [ ] æ ‡å‡†LoRAè®­ç»ƒï¼ˆå¯¹æ¯”åŸºå‡†ï¼‰
- [ ] Few-shotè¯„ä¼°ï¼ˆ0, 5, 10, 25, 50 shotï¼‰
- [ ] è®¡ç®—æ•ˆç‡å¯¹æ¯”ï¼ˆæ—¶é—´ã€å†…å­˜ï¼‰
- [ ] æ¶ˆèå®éªŒï¼ˆrank, steps, samplesï¼‰
- [ ] ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- [ ] ç»“æœå¯è§†åŒ–å’Œåˆ†æ

---

ç¥å®éªŒé¡ºåˆ©ï¼å¦‚æœMETA-LORAè¾¾åˆ°é¢„æœŸæ•ˆæœï¼Œè¿™å°†æ˜¯å…ƒå­¦ä¹ ä¸PEFTç»“åˆçš„ä¼˜ç§€æ¡ˆä¾‹ï¼ğŸš€
