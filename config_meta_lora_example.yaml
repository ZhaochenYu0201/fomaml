# META-LORA配置示例
# 基于论文: MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models

# Model配置
model:
  partial_pretrain: "meta-llama/Llama-3.2-1B"
  trust_remote_code: true

  # LoRA配置（关键！）
  lora_rank: 16  # r: LoRA秩
  lora_alpha: 32  # α: 缩放因子 (通常 α = 2r)
  lora_dropout: 0.05

  # LoRA target modules (根据模型架构调整)
  target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

# Meta-Learning配置
meta:
  # Stage 1: Task-specific adaptation
  inner_lr: 1e-4  # 任务适应学习率
  num_inner_steps: 10  # 适应步数（论文中可能5-20）
  inner_batch_size: 4

  # Stage 2: Shared LoRA update
  meta_lr: 5e-5  # 元学习率（通常比inner_lr小）
  meta_batch_size: 4  # 每次meta-update使用的任务数

  # 任务定义
  # 每个任务只需100个训练样本！
  tasks:
    - name: "algebra"
      train_files: ["data/meta_train/algebra_support.parquet"]  # 100 samples
      val_files: ["data/meta_train/algebra_query.parquet"]
      train_max_samples: 100  # ⭐ 关键：只用100样本
      val_max_samples: 200

    - name: "geometry"
      train_files: ["data/meta_train/geometry_support.parquet"]
      val_files: ["data/meta_train/geometry_query.parquet"]
      train_max_samples: 100
      val_max_samples: 200

    - name: "number_theory"
      train_files: ["data/meta_train/number_theory_support.parquet"]
      val_files: ["data/meta_train/number_theory_query.parquet"]
      train_max_samples: 100
      val_max_samples: 200

    - name: "word_problems"
      train_files: ["data/meta_train/word_problems_support.parquet"]
      val_files: ["data/meta_train/word_problems_query.parquet"]
      train_max_samples: 100
      val_max_samples: 200

    - name: "physics"
      train_files: ["data/meta_train/science_physics_support.parquet"]
      val_files: ["data/meta_train/science_physics_query.parquet"]
      train_max_samples: 100
      val_max_samples: 200

    - name: "chemistry"
      train_files: ["data/meta_train/science_chemistry_support.parquet"]
      val_files: ["data/meta_train/science_chemistry_query.parquet"]
      train_max_samples: 100
      val_max_samples: 200

# Data配置
data:
  max_length: 2048
  truncation: "right"
  prompt_key: "prompt"
  response_key: "response"

# Optimizer配置
optim:
  optimizer_type: "AdamW"
  weight_decay: 0.01
  clip_grad: 1.0
  betas: [0.9, 0.999]

# Trainer配置
trainer:
  device: "cuda"
  total_steps: 3000  # META-LORA收敛更快
  save_freq: 300
  test_freq: 100

  # Logging
  project_name: "meta-lora-math-science"
  experiment_name: "llama-3.2-1b-meta-lora"
  logger: "wandb"

  # Checkpoint
  default_local_dir: "./checkpoints/meta_lora"
  max_ckpt_to_keep: 5

# Checkpoint配置
checkpoint:
  save_contents: ["lora"]  # 只保存LoRA参数
  load_contents: ["lora"]
