# å…ƒå­¦ä¹ å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ å¿«é€Ÿé€‰æ‹©å’Œå¼€å§‹å…ƒå­¦ä¹ å®éªŒï¼ˆFOMAML-SFTæˆ–META-LORAï¼‰ã€‚

âš ï¸ **é‡è¦è¯´æ˜**ï¼šFOMAMLæ˜¯**å…¨å‚æ•°ä¼˜åŒ–**ï¼ˆä¸ä½¿ç”¨LoRAï¼‰ï¼ŒMETA-LORAæ˜¯**LoRAå‚æ•°ä¼˜åŒ–**ã€‚è¯·æ ¹æ®èµ„æºé€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚

---

## ğŸ¯ å¿«é€Ÿé€‰æ‹©æ–¹æ³•

| ä½ çš„æƒ…å†µ | æ¨èæ–¹æ³• | åŸå›  |
|---------|---------|------|
| ğŸš€ åªæœ‰1-2å—GPU | **META-LORA** â­ | 30GB vs 70GB |
| âš¡ æƒ³å¿«é€ŸéªŒè¯æƒ³æ³• | **META-LORA** â­ | 4-6h vs 40-60h |
| ğŸ“Š æ•°æ®æœ‰é™ï¼ˆ<200æ ·æœ¬/ä»»åŠ¡ï¼‰| **META-LORA** â­ | åªéœ€100æ ·æœ¬/ä»»åŠ¡ |
| ğŸ’° æœ‰4Ã—A100ä¸”è¿½æ±‚æè‡´æ€§èƒ½ | FOMAML | å…¨å‚æ•°ä¼˜åŒ– |

**å¤§å¤šæ•°æƒ…å†µä¸‹æ¨èMETA-LORAï¼** é™¤éä½ æœ‰å……è¶³èµ„æºä¸”è¿½æ±‚æè‡´æ€§èƒ½ã€‚

---

## ğŸ“š æ–‡æ¡£ç´¢å¼•

| æ–‡æ¡£ | å†…å®¹ | é€‚ç”¨äººç¾¤ |
|------|------|----------|
| **[README.md](README.md)** | é¡¹ç›®æ€»è§ˆã€å¿«é€Ÿå¼€å§‹ | æ‰€æœ‰äºº |
| **æœ¬æ–‡æ¡£** | å¿«é€Ÿé€‰æ‹©å’Œå¼€å§‹æŒ‡å— | æ–°æ‰‹å¿«é€Ÿå…¥é—¨ â­ |
| **[RUN_META_LORA.md](RUN_META_LORA.md)** | META-LORAè¯¦ç»†è¿è¡ŒæŒ‡å— | ä½¿ç”¨META-LORAçš„ç ”ç©¶è€… â­ |
| **[META_LORA_VS_FOMAML_COMPARISON.md](META_LORA_VS_FOMAML_COMPARISON.md)** | è¯¦ç»†å¯¹æ¯”å®éªŒè®¾è®¡ | è®¾è®¡å¯¹æ¯”å®éªŒçš„ç ”ç©¶è€… |
| **[FOMAML_FULL_PARAM_VS_LORA.md](FOMAML_FULL_PARAM_VS_LORA.md)** | å…¨å‚æ•° vs LoRAæ¾„æ¸… | æƒ³ç†è§£å®ç°å·®å¼‚çš„äºº |
| **[FOMAML_IMPLEMENTATION_DETAILS.md](FOMAML_IMPLEMENTATION_DETAILS.md)** | FOMAMLè¯¦ç»†å®ç°è®²è§£ | æ·±å…¥äº†è§£FOMAMLçš„ç ”ç©¶è€… |
| **[EXPERIMENT_DESIGN_MATH_SCIENCE.md](EXPERIMENT_DESIGN_MATH_SCIENCE.md)** | å®Œæ•´å®éªŒè®¾è®¡æ–¹æ¡ˆ | è®¾è®¡å®éªŒçš„ç ”ç©¶è€… |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆA: META-LORAï¼ˆæ¨èç»™å¤§å¤šæ•°ç”¨æˆ·ï¼‰â­

#### å‰ç½®æ¡ä»¶
```bash
# ç¡¬ä»¶ - å•å¡å³å¯ï¼
- GPU: 1Ã—A100 (80GB) æˆ–åŒç­‰ç®—åŠ›
- å­˜å‚¨: ~50GB

# è½¯ä»¶
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+
- pip install peft  # META-LORAéœ€è¦
```

#### 3æ­¥å¼€å§‹

```bash
# 1. å‡†å¤‡æ•°æ®ï¼ˆ100æ ·æœ¬/ä»»åŠ¡ï¼‰
python prepare_math_science_data.py \
    --output-dir ./data/math_science_meta \
    --support-ratio 0.15 \
    --query-ratio 0.25

# 2. è®­ç»ƒMETA-LORAï¼ˆå•å¡å³å¯ï¼‰
python meta_lora_trainer.py --config-name config_meta_lora_example

# 3. è¯„ä¼°
python evaluate_few_shot.py \
    --model-path ./checkpoints/meta_lora/meta_lora_checkpoint_step_3000.pt \
    --model-type meta_lora \
    --n-shots 0 5 10 25 50

# å®Œæˆï¼é¢„æœŸæ—¶é—´ï¼š8-10å°æ—¶ï¼ˆå•å¡A100ï¼‰
```

**META-LORAä¼˜åŠ¿ï¼š**
- âœ… åªéœ€30GBå†…å­˜ï¼ˆå•å¡å¤Ÿç”¨ï¼‰
- âœ… è®­ç»ƒæ—¶é—´4-6å°æ—¶ï¼ˆå¤šå¡ï¼‰
- âœ… åªéœ€100æ ·æœ¬/ä»»åŠ¡
- âœ… Checkpointåªæœ‰~10MB

è¯¦è§ï¼š**[RUN_META_LORA.md](RUN_META_LORA.md)**

---

### æ–¹æ¡ˆB: FOMAMLï¼ˆé«˜çº§ç”¨æˆ·ï¼Œè¿½æ±‚æè‡´æ€§èƒ½ï¼‰

#### å‰ç½®æ¡ä»¶
```bash
# ç¡¬ä»¶ - å¿…é¡»å¤šå¡ï¼
- GPU: 4Ã—A100 (80GB) æˆ–åŒç­‰ç®—åŠ›
- å­˜å‚¨: ~100GB

# è½¯ä»¶
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+
```

#### è¿è¡Œ

```bash
# 1. å‡†å¤‡æ•°æ®ï¼ˆ300æ ·æœ¬/ä»»åŠ¡ï¼‰
python prepare_math_science_data.py \
    --output-dir ./data/math_science_meta \
    --support-ratio 0.30 \
    --query-ratio 0.40

# 2. è®­ç»ƒFOMAMLï¼ˆå¿…é¡»å¤šå¡ï¼‰
torchrun --nproc_per_node=4 \
    maml_sft_trainer.py \
    --config-name config_maml_sft_example

# 3. è¯„ä¼°
python evaluate_few_shot.py \
    --model-path ./checkpoints/maml_sft/step_5000 \
    --model-type fomaml \
    --n-shots 0 5 10 25 50

# å®Œæˆï¼é¢„æœŸæ—¶é—´ï¼š40-60å°æ—¶ï¼ˆ4Ã—A100ï¼‰
```

**FOMAMLç‰¹ç‚¹ï¼š**
- âš ï¸ éœ€è¦70GBå†…å­˜ï¼ˆ4Ã—A100é…ç½®ï¼‰
- âš ï¸ è®­ç»ƒæ—¶é—´40-60å°æ—¶
- âš ï¸ éœ€è¦300æ ·æœ¬/ä»»åŠ¡
- âœ… å…¨å‚æ•°ä¼˜åŒ–ï¼Œç†è®ºä¸Šæ€§èƒ½æœ€ä½³

---

## ğŸ“– ç†è§£å…ƒå­¦ä¹ å®ç°çš„æ ¸å¿ƒ

### 0. FOMAML vs META-LORAçš„æœ¬è´¨åŒºåˆ«

```python
# FOMAMLï¼ˆå…¨å‚æ•°ä¼˜åŒ–ï¼‰
grads = torch.autograd.grad(
    support_loss,
    model.parameters(),  # â† å…¨éƒ¨1.2Bå‚æ•°
    create_graph=False,
)
for param, grad in zip(model.parameters(), grads):
    param = param - inner_lr * grad  # æ›´æ–°æ‰€æœ‰å‚æ•°

# META-LORAï¼ˆåªä¼˜åŒ–LoRAå‚æ•°ï¼‰
for param in base_model.parameters():
    param.requires_grad = False  # â† å†»ç»“base model

lora_model = get_peft_model(base_model, lora_config)
lora_params = [p for p in lora_model.parameters() if p.requires_grad]  # åªæœ‰~1.2Må‚æ•°

grads = torch.autograd.grad(support_loss, lora_params)
for param, grad in zip(lora_params, grads):
    param = param - inner_lr * grad  # åªæ›´æ–°LoRAå‚æ•°

# ç»“æœï¼š
# - META-LORAè®­ç»ƒå¿«10-100å€
# - META-LORAå†…å­˜å ç”¨å°‘50%+
# - META-LORAåªéœ€100æ ·æœ¬/ä»»åŠ¡
# - ä½†æ€§èƒ½ç•¥ä½äºFOMAMLï¼ˆå·®è·<5%ï¼‰
```

### 1. FOMAML vs MAMLçš„å…³é”®åŒºåˆ«

```python
# MAML (äºŒé˜¶)
grads = torch.autograd.grad(
    support_loss,
    model.parameters(),
    create_graph=True,      # â† ä¿ç•™è®¡ç®—å›¾ç”¨äºäºŒé˜¶å¯¼æ•°
    retain_graph=True,
)

# FOMAML (ä¸€é˜¶) - æˆ‘ä»¬çš„å®ç°
grads = torch.autograd.grad(
    support_loss,
    model.parameters(),
    create_graph=False,     # â† ä¸ä¿ç•™è®¡ç®—å›¾ â­
    retain_graph=False,
)

# ç»“æœï¼š
# - FOMAMLèŠ‚çœ50%å†…å­˜
# - FOMAMLèŠ‚çœ50%æ—¶é—´
# - FOMAMLæ€§èƒ½â‰ˆ95% MAML
```

### 2. ä¸verl SFTçš„å®Œç¾å…¼å®¹

```python
# FOMAMLä½¿ç”¨ç›¸åŒçš„SFTæŸå¤±è®¡ç®—
def _compute_sft_loss(self, batch, model):
    # è¿™ä¸ verl/workers/roles/utils/losses.py:sft_loss å®Œå…¨ä¸€è‡´
    loss_mask = batch["loss_mask"]  # åªå¯¹responseè®¡ç®—loss
    loss = -masked_sum(log_prob, loss_mask) / num_tokens
    return loss

# ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ ¼å¼
from verl.utils.dataset import SFTDataset  # ç›´æ¥å¤ç”¨
```

### 3. åŒå¾ªç¯ç»“æ„

```python
# å¤–å¾ªç¯ï¼šé‡‡æ ·ä»»åŠ¡æ‰¹æ¬¡
for task_batch in meta_iterations:

    # å¯¹æ¯ä¸ªä»»åŠ¡
    for task in task_batch:

        # å†…å¾ªç¯ï¼šåœ¨support setä¸Šé€‚åº”Kæ­¥
        Î¸_adapted = Î¸_meta
        for k in range(K):
            loss = sft_loss(support_batch, Î¸_adapted)
            grad = compute_grad(loss)
            Î¸_adapted = Î¸_adapted - Î± * grad  # é€‚åº”

        # å¤–å¾ªç¯ï¼šåœ¨query setä¸Šè®¡ç®—å…ƒæŸå¤±
        meta_loss += sft_loss(query_batch, Î¸_adapted)

    # å…ƒå‚æ•°æ›´æ–°
    Î¸_meta = Î¸_meta - Î² * âˆ‡meta_loss
```

---

## ğŸ“Š å®éªŒè®¾è®¡è¦ç‚¹

### æ•°æ®åˆ’åˆ†ç­–ç•¥

```
MATHæ•°æ®é›† â†’ æŒ‰é¢†åŸŸåˆ’åˆ†ä»»åŠ¡
â”œâ”€â”€ Algebra (Task 1)
â”‚   â”œâ”€â”€ Support: 300 samples (å†…å¾ªç¯é€‚åº”)
â”‚   â”œâ”€â”€ Query:   450 samples (å…ƒæ¢¯åº¦è®¡ç®—)
â”‚   â””â”€â”€ Test:    750 samples (è¯„ä¼°)
â”œâ”€â”€ Geometry (Task 2)
â”‚   â””â”€â”€ ...
â””â”€â”€ ... (6-8ä¸ªä»»åŠ¡)

GSM8K â†’ Word Problemsä»»åŠ¡
ScienceQA â†’ Physics/Chemistryä»»åŠ¡
```

### ä¸ºä»€ä¹ˆè¿™æ ·åˆ’åˆ†ï¼Ÿ

1. **ä»»åŠ¡ç›¸å…³ä½†ä¸é‡å **ï¼šä»£æ•°å’Œå‡ ä½•éƒ½æ˜¯æ•°å­¦ï¼Œä½†è§£æ³•ä¸åŒ
2. **è¶³å¤Ÿå¤šæ ·æ€§**ï¼š6-8ä¸ªä»»åŠ¡è¶³ä»¥å­¦ä¹ é€šç”¨çš„æ•°å­¦æ¨ç†èƒ½åŠ›
3. **åˆç†çš„æ•°æ®é‡**ï¼šæ¯ä¸ªä»»åŠ¡300-750æ ·æœ¬ï¼Œæ—¢èƒ½è®­ç»ƒåˆä¸è¿‡æ‹Ÿåˆ

### è¯„ä¼°æ–¹æ¡ˆ

```python
Few-Shotè¯„ä¼°ä»»åŠ¡ (æœªåœ¨meta-trainingä¸­è§è¿‡):
â”œâ”€â”€ Hard Algebra (åªç”¨Level 5éš¾é¢˜)
â”œâ”€â”€ Calculus (å¦‚æœmeta-trainæ²¡ç”¨)
â”œâ”€â”€ TheoremQA (å®šç†è¯æ˜ - æ–°ä»»åŠ¡ç±»å‹)
â””â”€â”€ MMLU-STEM (è·¨é¢†åŸŸè¿ç§»)

å¯¹æ¯ä¸ªä»»åŠ¡è¯„ä¼°: 0, 5, 10, 25, 50 shot
é‡å¤5æ¬¡å–å¹³å‡
```

---

## ğŸ¯ é¢„æœŸç»“æœ

### æˆåŠŸçš„å®éªŒåº”è¯¥çœ‹åˆ°ï¼š

**1. Few-Shotå­¦ä¹ æ›²çº¿**

```
Accuracy (%)
   80 â”¤                        â—â”€â”€â”€â— FOMAML-SFT
      â”‚                    â—â”€â”€â—
   60 â”¤               â—â”€â”€â—          â—‹â”€â”€â”€â—‹ Baseline SFT
      â”‚          â—â”€â”€â—           â—‹â”€â”€â—‹
   40 â”¤     â—â”€â”€â—           â—‹â”€â”€â—‹
      â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> N-shot
          0   5  10  25  50  100

å…³é”®è§‚å¯Ÿ:
- FOMAMLèµ·ç‚¹æ›´é«˜ (better zero-shot)
- FOMAMLä¸Šå‡æ›´å¿« (better few-shot learning)
- FOMAMLåœ¨10-shotæ—¶å°±èƒ½è¾¾åˆ°SFTçš„50-shotæ€§èƒ½
```

**2. æ ·æœ¬æ•ˆç‡æå‡**

```
è¾¾åˆ°70%å‡†ç¡®ç‡æ‰€éœ€æ ·æœ¬ï¼š
- FOMAML-SFT: 10 samples
- Baseline SFT: 30 samples
- æ•ˆç‡æå‡: 3x

è¿™æ­£æ˜¯meta-learningçš„ä»·å€¼ï¼
```

**3. Meta-TrainingæŒ‡æ ‡**

```
è®­ç»ƒè¿‡ç¨‹ä¸­åº”è¯¥çœ‹åˆ°ï¼š
- meta/loss æŒç»­ä¸‹é™
- adaptation_gap é€æ¸å‡å°
  (query_loss - support_loss â†’ 0)

adaptation_gapå‡å° = å…ƒåˆå§‹åŒ–è¶Šæ¥è¶Šå¥½
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

**é¦–é€‰æ–¹æ¡ˆï¼šä½¿ç”¨META-LORAï¼**

META-LORAåªéœ€30GBå†…å­˜ï¼ˆvs FOMAMLçš„70GBï¼‰ã€‚

å¦‚æœè¿˜ä¸å¤Ÿï¼š
```yaml
# META-LORAè¿›ä¸€æ­¥ä¼˜åŒ–
meta:
  inner_batch_size: 2
  meta_batch_size: 2
model:
  lora_rank: 8  # ä»16é™åˆ°8

# FOMAMLä¼˜åŒ–ï¼ˆä¸æ¨èï¼Œå»ºè®®ç›´æ¥æ¢META-LORAï¼‰
meta:
  inner_batch_size: 2
  meta_batch_size: 2
  num_inner_steps: 3
```

### Q2: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

```bash
# æ–¹æ¡ˆ1: å‡å°‘è®­ç»ƒæ­¥æ•°
trainer:
  total_steps: 3000  # ä»5000é™åˆ°3000

# æ–¹æ¡ˆ2: å‡å°‘ä»»åŠ¡æ•°
meta:
  tasks: [åªä¿ç•™5-6ä¸ªæ ¸å¿ƒä»»åŠ¡]

# æ–¹æ¡ˆ3: ä½¿ç”¨Reptile (æ›´ç®€å•å¿«é€Ÿçš„ç®—æ³•)
python reptile_sft_trainer.py
```

### Q3: æ•ˆæœä¸å¦‚é¢„æœŸæ€ä¹ˆåŠï¼Ÿ

```python
# è¯Šæ–­æ­¥éª¤:

# 1. æ£€æŸ¥ä»»åŠ¡ç›¸å…³æ€§
# ä»»åŠ¡æ˜¯å¦çœŸçš„ç›¸å…³ï¼Ÿæ˜¯å¦éƒ½æ˜¯æ¨ç†ä»»åŠ¡ï¼Ÿ

# 2. æ£€æŸ¥adaptation_gap
# æ˜¯å¦åœ¨ä¸‹é™ï¼Ÿå¦‚æœä¸ä¸‹é™è¯´æ˜meta-learningæ²¡å­¦å¥½

# 3. è°ƒæ•´è¶…å‚æ•°
inner_lr: [1e-5, 5e-5, 1e-4]      # è¯•3ä¸ªå€¼
num_inner_steps: [3, 5, 10]       # è¯•3ä¸ªå€¼
outer_lr: ä¿æŒ inner_lr / 3-5

# 4. å¢åŠ è®­ç»ƒæ—¶é—´
total_steps: 5000 â†’ 10000
```

---

## ğŸ“ é¡¹ç›®æ–‡ä»¶ç»“æ„

```
meta_learning/
â”œâ”€â”€ README.md                              # é¡¹ç›®æ€»è§ˆ
â”œâ”€â”€ QUICK_START_GUIDE.md                   # æœ¬æ–‡æ¡£ â­
â”œâ”€â”€ RUN_META_LORA.md                       # META-LORAè¯¦ç»†æŒ‡å— â­
â”‚
â”œâ”€â”€ meta_lora_trainer.py                   # META-LORAè®­ç»ƒå™¨ï¼ˆæ¨èï¼‰â­
â”œâ”€â”€ maml_sft_trainer.py                    # FOMAMLè®­ç»ƒå™¨ï¼ˆå…¨å‚æ•°ï¼‰
â”œâ”€â”€ reptile_sft_trainer.py                 # Reptileè®­ç»ƒå™¨
â”‚
â”œâ”€â”€ config_meta_lora_example.yaml          # META-LORAé…ç½® â­
â”œâ”€â”€ config_maml_sft_example.yaml           # FOMAMLé…ç½®
â”‚
â”œâ”€â”€ prepare_math_science_data.py           # æ•°æ®å‡†å¤‡è„šæœ¬ â­
â”œâ”€â”€ evaluate_few_shot.py                   # Few-shotè¯„ä¼°è„šæœ¬ â­
â”‚
â”œâ”€â”€ FOMAML_FULL_PARAM_VS_LORA.md          # å…¨å‚æ•° vs LoRAè¯´æ˜
â”œâ”€â”€ META_LORA_VS_FOMAML_COMPARISON.md     # è¯¦ç»†å¯¹æ¯”æŒ‡å—
â”œâ”€â”€ FOMAML_IMPLEMENTATION_DETAILS.md       # FOMAMLå®ç°è¯¦è§£
â”œâ”€â”€ EXPERIMENT_DESIGN_MATH_SCIENCE.md      # å®éªŒè®¾è®¡
â”‚
â””â”€â”€ verl/                                  # verlæ¡†æ¶
    â””â”€â”€ trainer/
        â”œâ”€â”€ sft_trainer.py                 # verl SFTè®­ç»ƒå™¨
        â””â”€â”€ fsdp_sft_trainer.py            # verl FSDP SFTè®­ç»ƒå™¨
```

---

## âš¡ æœ€å°åŒ–å®éªŒï¼ˆå¿«é€ŸéªŒè¯ï¼‰

### æ–¹æ¡ˆA: META-LORAæœ€å°åŒ–ï¼ˆæ¨èï¼‰

```bash
# 1. åªç”¨2ä¸ªä»»åŠ¡
python prepare_math_science_data.py \
    --output-dir ./data/mini_experiment \
    --tasks algebra geometry  # åªç”¨2ä¸ªä»»åŠ¡

# 2. ä¿®æ”¹é…ç½®
# config_meta_lora_example.yaml:
meta:
  tasks: [algebra, geometry]  # åªç”¨2ä¸ªä»»åŠ¡
  num_inner_steps: 5
trainer:
  total_steps: 1000  # å‡å°‘è®­ç»ƒæ­¥æ•°

# 3. å¿«é€Ÿè®­ç»ƒï¼ˆå•å¡å³å¯ï¼ï¼‰
python meta_lora_trainer.py --config-name config_meta_lora_example

# 4. ç®€åŒ–è¯„ä¼°
python evaluate_few_shot.py \
    --model-type meta_lora \
    --eval-tasks algebra \
    --n-shots 0 10 50 \
    --n-runs 1

# æ€»æ—¶é—´: ~2-3å°æ—¶ï¼ˆå•å¡A100ï¼‰âœ…
```

### æ–¹æ¡ˆB: FOMAMLæœ€å°åŒ–ï¼ˆéœ€è¦å¤šå¡ï¼‰

```bash
# 1-2æ­¥åŒä¸Š

# 3. å¿«é€Ÿè®­ç»ƒï¼ˆè‡³å°‘2å¡ï¼‰
torchrun --nproc_per_node=2 maml_sft_trainer.py

# 4. ç®€åŒ–è¯„ä¼°
python evaluate_few_shot.py \
    --model-type fomaml \
    --eval-tasks algebra \
    --n-shots 0 10 50 \
    --n-runs 1

# æ€»æ—¶é—´: ~6-8å°æ—¶ï¼ˆ2Ã—A100ï¼‰
```

**æ¨èMETA-LORAæ–¹æ¡ˆï¼šæ›´å¿«ï¼Œå•å¡å³å¯ï¼**

---

## ğŸ“ˆ å®éªŒé‡Œç¨‹ç¢‘

| é˜¶æ®µ | æ—¶é—´ | å®Œæˆæ ‡å¿— | å¯ä»¥å¼€å§‹ä¸‹ä¸€æ­¥çš„æ¡ä»¶ |
|------|------|----------|---------------------|
| **æ•°æ®å‡†å¤‡** | 1-2å¤© | ç”Ÿæˆæ‰€æœ‰parquetæ–‡ä»¶å’Œé…ç½® | æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡ |
| **Baselineè®­ç»ƒ** | 2-3å¤© | train/loss < 1.0 | checkpointä¿å­˜æˆåŠŸ |
| **FOMAMLè®­ç»ƒ** | 3-5å¤© | adaptation_gapä¸‹é™ | checkpointä¿å­˜æˆåŠŸ |
| **è¯„ä¼°** | 2-3å¤© | æ‰€æœ‰ä»»åŠ¡è¯„ä¼°å®Œæˆ | ç”Ÿæˆå­¦ä¹ æ›²çº¿å›¾ |
| **åˆ†æ** | 1-2å¤© | ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒå®Œæˆ | å†™å‡ºå®éªŒæŠ¥å‘Š |

---

## ğŸ“ å­¦ä¹ è·¯å¾„

### Level 1: ç†è§£æ¦‚å¿µ
1. é˜…è¯» [README.md](README.md) äº†è§£é¡¹ç›®
2. é˜…è¯» [EXPERIMENT_DESIGN_MATH_SCIENCE.md](EXPERIMENT_DESIGN_MATH_SCIENCE.md) ç¬¬1-2èŠ‚
3. ç†è§£MAML vs FOMAMLçš„åŒºåˆ«

### Level 2: è¿è¡Œå®éªŒ
1. å‡†å¤‡ç¯å¢ƒå’Œæ•°æ®
2. è·Ÿéš [EXPERIMENT_RUNBOOK.md](EXPERIMENT_RUNBOOK.md) è¿è¡Œå®éªŒ
3. è§‚å¯Ÿè®­ç»ƒæŒ‡æ ‡

### Level 3: æ·±å…¥å®ç°
1. é˜…è¯» [FOMAML_IMPLEMENTATION_DETAILS.md](FOMAML_IMPLEMENTATION_DETAILS.md)
2. ç†è§£åŒå¾ªç¯ç»“æ„å’Œæ¢¯åº¦è®¡ç®—
3. é˜…è¯» `maml_sft_trainer.py` æºç 

### Level 4: ä¼˜åŒ–å’Œæ‰©å±•
1. è°ƒè¯•è¶…å‚æ•°
2. å°è¯•æ–°ä»»åŠ¡ç»„åˆ
3. å®ç°è‡ªå·±çš„å˜ä½“

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### ä¸ºä»€ä¹ˆFOMAMLæœ‰æ•ˆï¼Ÿ

1. **å…ƒå­¦ä¹ çš„æœ¬è´¨**ï¼šå­¦ä¹ ä¸€ä¸ª"å¥½çš„åˆå§‹åŒ–"ï¼Œä½¿æ¨¡å‹èƒ½å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
2. **ä¸€é˜¶è¿‘ä¼¼è¶³å¤Ÿå¥½**ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒHessian â‰ˆ I
3. **è®¡ç®—æ•ˆç‡å…³é”®**ï¼šå¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œ50%çš„åŠ é€Ÿå¾ˆé‡è¦

### FOMAML vs SFTçš„æœ¬è´¨åŒºåˆ«

```
Baseline SFT:
ç›®æ ‡: min Loss(æ‰€æœ‰æ•°æ®æ··åˆ)
ç»“æœ: åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå¹³å‡æ€§èƒ½å¥½
ç¼ºç‚¹: éš¾ä»¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡

FOMAML-SFT:
ç›®æ ‡: min Î£_tasks Loss_query(adapt_K_steps(task))
ç»“æœ: å­¦åˆ°æ˜“äºé€‚åº”çš„åˆå§‹åŒ–
ä¼˜åŠ¿: åœ¨æ–°ä»»åŠ¡ä¸Šå¿«é€Ÿæ”¶æ•›
```

### ä½•æ—¶åº”è¯¥ç”¨FOMAMLï¼Ÿ

âœ… **é€‚åˆçš„åœºæ™¯ï¼š**
- æœ‰å¤šä¸ªç›¸å…³ä»»åŠ¡çš„æ•°æ®
- éœ€è¦å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
- å¸Œæœ›æå‡few-shotæ€§èƒ½
- æ•°æ®é‡æœ‰é™

âŒ **ä¸é€‚åˆçš„åœºæ™¯ï¼š**
- åªæœ‰ä¸€ä¸ªä»»åŠ¡
- æ•°æ®é‡å……è¶³
- ä»»åŠ¡é—´å®Œå…¨æ— å…³
- åªå…³å¿ƒå•ä»»åŠ¡æ€§èƒ½

---

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [MAML (Finn et al., ICML 2017)](https://arxiv.org/abs/1703.03400)
- [FOMAML](https://arxiv.org/abs/1803.02999)
- [Reptile (Nichol et al., 2018)](https://arxiv.org/abs/1803.02999)

### ä»£ç 
- [verlæ¡†æ¶](https://github.com/volcengine/verl)
- [learn2learn](https://github.com/learnables/learn2learn)
- [higher](https://github.com/facebookresearch/higher)

### æ•°æ®é›†
- [MATH](https://github.com/hendrycks/math)
- [GSM8K](https://github.com/openai/grade-school-math)
- [ScienceQA](https://scienceqa.github.io/)

---

## ğŸ“§ è·å¾—å¸®åŠ©

é‡åˆ°é—®é¢˜æ—¶ï¼š

1. **æ£€æŸ¥æ–‡æ¡£**ï¼š
   - å®ç°é—®é¢˜ â†’ [FOMAML_IMPLEMENTATION_DETAILS.md](FOMAML_IMPLEMENTATION_DETAILS.md)
   - å®éªŒé—®é¢˜ â†’ [EXPERIMENT_RUNBOOK.md](EXPERIMENT_RUNBOOK.md)

2. **æ£€æŸ¥æ—¥å¿—**ï¼š
   ```bash
   # æŸ¥çœ‹wandb dashboard
   # å…³æ³¨ meta/loss, adaptation_gap ç­‰æŒ‡æ ‡
   ```

3. **è°ƒè¯•æ¨¡å¼**ï¼š
   ```python
   # åœ¨ä»£ç ä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
   print(f"Support loss: {support_loss:.4f}")
   print(f"Query loss: {query_loss:.4f}")
   print(f"Adaptation gap: {query_loss - support_loss:.4f}")
   ```

---

**ç¥å®éªŒé¡ºåˆ©ï¼ğŸš€**

æœ‰é—®é¢˜éšæ—¶å‚è€ƒè¯¦ç»†æ–‡æ¡£æˆ–åœ¨é¡¹ç›®ä¸­æissueã€‚
