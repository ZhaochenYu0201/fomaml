# MAML-SFT Implementation Guide for LLMs

åŸºäºverlæ¡†æ¶çš„å¤§è¯­è¨€æ¨¡å‹å…ƒå­¦ä¹ ç›‘ç£å¾®è°ƒå®ç°æŒ‡å—

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ç†è®ºèƒŒæ™¯](#ç†è®ºèƒŒæ™¯)
3. [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
4. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
5. [ä¼˜åŒ–æŠ€å·§](#ä¼˜åŒ–æŠ€å·§)
6. [å®éªŒå»ºè®®](#å®éªŒå»ºè®®)

---

## æ¦‚è¿°

### ä»€ä¹ˆæ˜¯MAML-SFTï¼Ÿ

MAML-SFTå°†Model-Agnostic Meta-Learning (MAML)åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒ(SFT)è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿï¼š

- **å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡**ï¼šåœ¨æ–°é¢†åŸŸåªéœ€å°‘é‡æ ·æœ¬å³å¯fine-tune
- **è·¨é¢†åŸŸæ³›åŒ–**ï¼šå­¦ä¹ åˆ°é€šç”¨çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
- **ä¸ªæ€§åŒ–**ï¼šä¸ºä¸åŒç”¨æˆ·/åœºæ™¯å¿«é€Ÿå®šåˆ¶æ¨¡å‹

### æ ¸å¿ƒåŒºåˆ«ï¼šä¼ ç»ŸSFT vs MAML-SFT

| ç‰¹æ€§ | ä¼ ç»ŸSFT | MAML-SFT |
|------|---------|----------|
| è®­ç»ƒç›®æ ‡ | åœ¨ç‰¹å®šä»»åŠ¡ä¸Šæœ€å°åŒ–æŸå¤± | å­¦ä¹ æ˜“äºé€‚åº”çš„åˆå§‹åŒ– |
| æ•°æ®éœ€æ±‚ | å•ä¸€ä»»åŠ¡å¤§é‡æ•°æ® | å¤šä»»åŠ¡å°‘é‡æ•°æ® |
| æ³›åŒ–èƒ½åŠ› | å•ä»»åŠ¡æ€§èƒ½å¥½ | è·¨ä»»åŠ¡é€‚åº”å¿« |
| è®­ç»ƒå¤æ‚åº¦ | O(N) | O(N Ã— K) (Kä¸ºå†…å¾ªç¯æ­¥æ•°) |

---

## ç†è®ºèƒŒæ™¯

### MAMLç®—æ³•æµç¨‹

```
åˆå§‹åŒ–ï¼šå…ƒå‚æ•° Î¸

å¯¹äºæ¯ä¸ªmeta-iteration:
    1. é‡‡æ ·ä»»åŠ¡æ‰¹æ¬¡ {Tâ‚, Tâ‚‚, ..., Tâ‚˜}

    2. å¯¹æ¯ä¸ªä»»åŠ¡ Táµ¢:
        a. å†…å¾ªç¯ï¼ˆä»»åŠ¡é€‚åº”ï¼‰ï¼š
           ä» support set Dáµ¢Ë¢áµ˜áµ–áµ–áµ’Ê³áµ— é‡‡æ ·æ•°æ®
           æ‰§è¡Œ K æ­¥æ¢¯åº¦ä¸‹é™ï¼š
           Î¸'áµ¢ = Î¸ - Î±âˆ‡Î¸ L_Táµ¢(f_Î¸; Dáµ¢Ë¢áµ˜áµ–áµ–áµ’Ê³áµ—)

        b. å¤–å¾ªç¯ï¼ˆå…ƒå­¦ä¹ ï¼‰ï¼š
           ä» query set Dáµ¢qáµ˜áµ‰Ê³Ê¸ é‡‡æ ·æ•°æ®
           è®¡ç®—å…ƒæŸå¤±ï¼š
           meta_lossáµ¢ = L_Táµ¢(f_Î¸'áµ¢; Dáµ¢qáµ˜áµ‰Ê³Ê¸)

    3. å…ƒå‚æ•°æ›´æ–°ï¼š
       Î¸ = Î¸ - Î²âˆ‡Î¸ Î£áµ¢ meta_lossáµ¢
```

### FOMAMLç®€åŒ–

FOMAMLå¿½ç•¥äºŒé˜¶æ¢¯åº¦ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼š

```python
# MAML (äºŒé˜¶)
meta_grad = grad(meta_loss, Î¸)  # éœ€è¦è®¡ç®— d(Î¸')/d(Î¸)

# FOMAML (ä¸€é˜¶)
meta_grad = grad(meta_loss, Î¸')  # ç›´æ¥ä½¿ç”¨ä¸€é˜¶æ¢¯åº¦
```

å¯¹äºLLMç­‰å¤§æ¨¡å‹ï¼Œ**æ¨èä½¿ç”¨FOMAML**ï¼š
- å†…å­˜å¼€é”€å°ï¼ˆä¸éœ€è¦å­˜å‚¨å®Œæ•´è®¡ç®—å›¾ï¼‰
- é€Ÿåº¦å¿«ï¼ˆé¿å…äºŒé˜¶æ¢¯åº¦è®¡ç®—ï¼‰
- æ€§èƒ½æ¥è¿‘å®Œæ•´MAML

---

## å®ç°ç»†èŠ‚

### 1. verl SFTå®ç°åˆ†æ

#### æ ¸å¿ƒç»„ä»¶

```
verl/trainer/sft_trainer.py (æ ‡å‡†è®­ç»ƒå™¨)
â”œâ”€â”€ _build_engine()          # æ„å»ºè®­ç»ƒå¼•æ“
â”œâ”€â”€ _build_dataset()         # æ„å»ºæ•°æ®é›†
â”œâ”€â”€ _build_dataloader()      # æ„å»ºæ•°æ®åŠ è½½å™¨
â””â”€â”€ fit()                    # è®­ç»ƒå¾ªç¯

verl/trainer/fsdp_sft_trainer.py (FSDPè®­ç»ƒå™¨)
â”œâ”€â”€ _build_model_optimizer() # æ„å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
â”œâ”€â”€ training_step()          # å•æ­¥è®­ç»ƒ
â”œâ”€â”€ _compute_loss_and_backward() # æŸå¤±è®¡ç®—
â””â”€â”€ fit()                    # è®­ç»ƒå¾ªç¯

verl/workers/roles/utils/losses.py
â””â”€â”€ sft_loss()              # SFTæŸå¤±å‡½æ•°
```

#### æŸå¤±è®¡ç®—æœºåˆ¶

```python
# verlçš„SFTæŸå¤±è®¡ç®— (losses.py:27-53)
def sft_loss(config, model_output, data, dp_group=None):
    log_prob = model_output["log_probs"]
    loss_mask = data["loss_mask"]  # åªå¯¹responseè®¡ç®—loss

    # å…³é”®ï¼šmasked sumï¼Œåªè®¡ç®—æœ‰æ•ˆtoken
    loss = -masked_sum(log_prob, loss_mask) / batch_num_tokens

    return loss, {"loss": loss.detach().item()}
```

**é‡è¦ç‰¹æ€§ï¼š**
1. **Masked Loss**: åªå¯¹responseéƒ¨åˆ†è®¡ç®—æŸå¤±ï¼Œpromptè¢«mask
2. **Token Normalization**: æŸå¤±é™¤ä»¥æœ‰æ•ˆtokenæ•°
3. **æ•°æ®å¹¶è¡Œ**: è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒ

#### æ•°æ®æ ¼å¼

```python
# sft_dataset.pyè¿”å›çš„æ•°æ®æ ¼å¼
{
    'input_ids': torch.Tensor,      # [seq_len]
    'attention_mask': torch.Tensor, # [seq_len]
    'position_ids': torch.Tensor,   # [seq_len]
    'loss_mask': torch.Tensor,      # [seq_len], promptéƒ¨åˆ†ä¸º0
}
```

### 2. MAML-SFTå…³é”®å®ç°

#### åŒå¾ªç¯ç»“æ„

```python
class MAMLSFTTrainer:
    def _meta_update_step(self, task_batch):
        meta_loss = 0.0

        # ä¿å­˜åŸå§‹å‚æ•°
        original_params = clone_params(self.model)

        for task in task_batch:
            # === å†…å¾ªç¯ï¼šä»»åŠ¡é€‚åº” ===
            support_batch = sample_support(task)

            # Kæ­¥æ¢¯åº¦ä¸‹é™
            for k in range(self.num_inner_steps):
                loss = self._compute_sft_loss(support_batch)
                grads = compute_gradients(loss)
                update_params(grads, lr=self.inner_lr)

            # === å¤–å¾ªç¯ï¼šå…ƒæŸå¤± ===
            query_batch = sample_query(task)
            query_loss = self._compute_sft_loss(query_batch)
            meta_loss += query_loss

            # æ¢å¤åŸå§‹å‚æ•°
            restore_params(original_params)

        # å…ƒæ¢¯åº¦æ›´æ–°
        meta_loss.backward()
        self.meta_optimizer.step()
```

#### æ¢¯åº¦è®¡ç®—ç»†èŠ‚

```python
def _inner_loop_update(self, support_batch):
    # å…‹éš†å½“å‰å‚æ•°
    fast_weights = {n: p.clone() for n, p in self.model.named_parameters()}

    for step in range(self.num_inner_steps):
        loss = self._compute_sft_loss(support_batch, self.model)

        # è®¡ç®—æ¢¯åº¦
        grads = torch.autograd.grad(
            loss,
            self.model.parameters(),
            create_graph=not self.use_fomaml,  # FOMAMLä¸éœ€è¦è®¡ç®—å›¾
            retain_graph=True,
        )

        # æ›´æ–°fast weights
        for (name, param), grad in zip(self.model.named_parameters(), grads):
            fast_weights[name] = fast_weights[name] - self.inner_lr * grad

        # åº”ç”¨fast weightsåˆ°æ¨¡å‹
        load_params(self.model, fast_weights)

    return fast_weights
```

---

## ä½¿ç”¨æ–¹æ³•

### Step 1: å‡†å¤‡æ•°æ®

æ¯ä¸ªä»»åŠ¡éœ€è¦ä¸¤ä¸ªæ•°æ®é›†ï¼š
- **Support Set**: ç”¨äºå†…å¾ªç¯ä»»åŠ¡é€‚åº”ï¼ˆå°‘é‡æ ·æœ¬ï¼Œå¦‚100-500æ¡ï¼‰
- **Query Set**: ç”¨äºå¤–å¾ªç¯å…ƒå­¦ä¹ ï¼ˆè¯„ä¼°æ ·æœ¬ï¼Œå¦‚500-1000æ¡ï¼‰

#### æ•°æ®æ ¼å¼ç¤ºä¾‹

```json
// medical_support.jsonl
{"prompt": "æ‚£è€…ä¸»è¯‰å¤´ç—›ï¼Œå¦‚ä½•è¯Šæ–­ï¼Ÿ", "response": "éœ€è¦è¯¢é—®..."}
{"prompt": "é«˜è¡€å‹çš„æ²»ç–—æ–¹æ¡ˆæœ‰å“ªäº›ï¼Ÿ", "response": "ä¸»è¦åŒ…æ‹¬..."}

// medical_query.jsonl
{"prompt": "ç³–å°¿ç—…æ‚£è€…çš„é¥®é£Ÿå»ºè®®", "response": "åº”è¯¥æ³¨æ„..."}
```

#### ä½¿ç”¨æ•°æ®å‡†å¤‡è„šæœ¬

```bash
# åˆ›å»ºä»»åŠ¡é…ç½®æ–‡ä»¶
cat > task_config.json << EOF
{
  "medical": {
    "input_file": "raw_data/medical.parquet",
    "support_ratio": 0.2,
    "max_samples": 5000
  },
  "legal": {
    "input_file": "raw_data/legal.parquet",
    "support_ratio": 0.2,
    "max_samples": 5000
  },
  "coding": {
    "input_file": "raw_data/coding.parquet",
    "support_ratio": 0.2,
    "max_samples": 5000
  }
}
EOF

# è¿è¡Œæ•°æ®å‡†å¤‡
python prepare_maml_data.py \
    --config task_config.json \
    --output-dir ./data/maml \
    --balance \
    --support-size 500 \
    --query-size 1000 \
    --verify
```

### Step 2: é…ç½®è®­ç»ƒå‚æ•°

ç¼–è¾‘ `config_maml_sft_example.yaml`:

```yaml
meta:
  use_fomaml: true  # æ¨èç”¨FOMAML

  # å†…å¾ªç¯å‚æ•°
  inner_lr: 1e-4     # ä»»åŠ¡é€‚åº”å­¦ä¹ ç‡
  num_inner_steps: 5 # é€‚åº”æ­¥æ•°ï¼ˆKï¼‰
  inner_batch_size: 4

  # å¤–å¾ªç¯å‚æ•°
  outer_lr: 3e-5     # å…ƒå­¦ä¹ ç‡
  meta_batch_size: 4 # æ¯æ¬¡meta-updateä½¿ç”¨å‡ ä¸ªä»»åŠ¡
  query_batch_size: 4

  # ä»»åŠ¡å®šä¹‰
  tasks:
    - name: "medical"
      support_files: ["data/maml/medical/support.parquet"]
      query_files: ["data/maml/medical/query.parquet"]
    # ... æ›´å¤šä»»åŠ¡
```

### Step 3: å¯åŠ¨è®­ç»ƒ

```bash
# å•å¡è®­ç»ƒ
python maml_sft_trainer.py

# å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=4 maml_sft_trainer.py

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python maml_sft_trainer.py --config-name my_maml_config
```

### Step 4: è¯„ä¼°å’Œéƒ¨ç½²

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”åˆ°æ–°ä»»åŠ¡ï¼š

```python
# åŠ è½½å…ƒå­¦ä¹ çš„æ¨¡å‹
model = load_checkpoint("checkpoints/maml_sft/step_10000.pt")

# åœ¨æ–°ä»»åŠ¡çš„å°‘é‡æ ·æœ¬ä¸Šfine-tune
new_task_data = load_data("new_task/support.parquet")  # åªéœ€10-50æ¡
optimizer = AdamW(model.parameters(), lr=1e-4)

for epoch in range(5):  # åªéœ€å‡ æ­¥
    for batch in new_task_data:
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()

# ç°åœ¨æ¨¡å‹å·²ç»é€‚åº”äº†æ–°ä»»åŠ¡ï¼
```

---

## ä¼˜åŒ–æŠ€å·§

### 1. å†…å­˜ä¼˜åŒ–

MAMLéœ€è¦å­˜å‚¨å¤šä¸ªä»»åŠ¡çš„æ¢¯åº¦ï¼Œå†…å­˜å¼€é”€å¤§ï¼š

```python
# æŠ€å·§1: ä½¿ç”¨FOMAMLè€Œä¸æ˜¯MAML
use_fomaml: true  # èŠ‚çœ50%+ å†…å­˜

# æŠ€å·§2: å‡å°‘meta_batch_size
meta_batch_size: 2  # ä»4é™åˆ°2

# æŠ€å·§3: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps: 2

# æŠ€å·§4: ä½¿ç”¨LoRA
model:
  lora_rank: 8
  target_modules: ["q_proj", "v_proj"]
```

### 2. é€Ÿåº¦ä¼˜åŒ–

```python
# æŠ€å·§1: å‡å°‘å†…å¾ªç¯æ­¥æ•°
num_inner_steps: 3  # ä»5é™åˆ°3ï¼Œé€šå¸¸å½±å“ä¸å¤§

# æŠ€å·§2: ä½¿ç”¨æ›´å°çš„å†…å¾ªç¯batch size
inner_batch_size: 2  # æ¯æ­¥æ›´å¿«

# æŠ€å·§3: ä½¿ç”¨Flash Attention
model:
  attn_implementation: "flash_attention_2"

# æŠ€å·§4: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model:
  enable_gradient_checkpointing: true
```

### 3. æ€§èƒ½ä¼˜åŒ–

```python
# æŠ€å·§1: è°ƒæ•´å­¦ä¹ ç‡æ¯”ä¾‹
# å†…å¾ªç¯å­¦ä¹ ç‡åº”è¯¥æ¯”å¤–å¾ªç¯å¤§
inner_lr: 1e-4
outer_lr: 3e-5  # inner_lr / outer_lr â‰ˆ 3-5

# æŠ€å·§2: ä½¿ç”¨é¢„è®­ç»ƒåˆå§‹åŒ–
# ä»å·²ç»SFTè¿‡çš„æ¨¡å‹å¼€å§‹ï¼Œè€Œä¸æ˜¯base model
model:
  partial_pretrain: "path/to/sft_checkpoint"

# æŠ€å·§3: å¹³è¡¡ä»»åŠ¡éš¾åº¦
# ç¡®ä¿å„ä»»åŠ¡çš„support setå¤§å°å’Œéš¾åº¦ç›¸è¿‘

# æŠ€å·§4: ä»»åŠ¡é‡‡æ ·ç­–ç•¥
# å¯ä»¥æ ¹æ®ä»»åŠ¡æŸå¤±åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¦‚ç‡
```

### 4. æ•°æ®ä¼˜åŒ–

```python
# æŠ€å·§1: Support setè´¨é‡ > æ•°é‡
# ç²¾å¿ƒæŒ‘é€‰æœ‰ä»£è¡¨æ€§çš„æ ·æœ¬ï¼Œ100æ¡é«˜è´¨é‡ > 1000æ¡ä½è´¨é‡

# æŠ€å·§2: Query setå¤šæ ·æ€§
# Query setåº”è¯¥æ¶µç›–ä»»åŠ¡çš„å„ä¸ªæ–¹é¢

# æŠ€å·§3: ä»»åŠ¡ç›¸å…³æ€§
# é€‰æ‹©ç›¸å…³ä½†ä¸é‡å çš„ä»»åŠ¡ï¼ˆå¦‚åŒ»ç–—å„å­é¢†åŸŸï¼‰

# æŠ€å·§4: æ•°æ®å¢å¼º
# å¯ä»¥å¯¹support setåšparaphraseç­‰å¢å¼º
```

---

## å®éªŒå»ºè®®

### åŸºå‡†å®éªŒè®¾è®¡

```python
å®éªŒ1ï¼šéªŒè¯MAMLæœ‰æ•ˆæ€§
â”œâ”€â”€ Baseline: æ ‡å‡†SFTåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè®­ç»ƒ
â”œâ”€â”€ MAML-SFT: å…ƒå­¦ä¹ è®­ç»ƒ
â””â”€â”€ è¯„ä¼°: åœ¨æ–°ä»»åŠ¡ä¸Šfew-shoté€‚åº”æ€§èƒ½

å®éªŒ2ï¼šMAML vs FOMAML
â”œâ”€â”€ MAML (äºŒé˜¶)
â”œâ”€â”€ FOMAML (ä¸€é˜¶)
â””â”€â”€ å¯¹æ¯”: æ€§èƒ½ã€é€Ÿåº¦ã€å†…å­˜

å®éªŒ3ï¼šè¶…å‚æ•°æ•æ„Ÿæ€§
â”œâ”€â”€ inner_lr: [1e-5, 1e-4, 1e-3]
â”œâ”€â”€ num_inner_steps: [1, 3, 5, 10]
â””â”€â”€ meta_batch_size: [2, 4, 8]
```

### è¯„ä¼°æŒ‡æ ‡

1. **Few-shoté€‚åº”é€Ÿåº¦**
   - åœ¨æ–°ä»»åŠ¡çš„N-shot (N=10, 50, 100)ä¸Šfine-tune
   - è®°å½•è¾¾åˆ°ç›®æ ‡æ€§èƒ½æ‰€éœ€çš„æ­¥æ•°

2. **è·¨ä»»åŠ¡æ³›åŒ–**
   - åœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šzero-shotæ€§èƒ½
   - ä¸base modelå¯¹æ¯”æå‡

3. **è®­ç»ƒæ•ˆç‡**
   - æ”¶æ•›é€Ÿåº¦
   - å†…å­˜å ç”¨
   - è®­ç»ƒæ—¶é—´

### å»ºè®®çš„ä»»åŠ¡ç»„åˆ

#### æ–¹æ¡ˆ1ï¼šå¤šé¢†åŸŸé€šç”¨
```yaml
tasks:
  - medical        # åŒ»ç–—å¯¹è¯
  - legal          # æ³•å¾‹å’¨è¯¢
  - coding         # ä»£ç ç”Ÿæˆ
  - math           # æ•°å­¦é—®é¢˜
  - creative       # åˆ›æ„å†™ä½œ
```

#### æ–¹æ¡ˆ2ï¼šå‚ç›´é¢†åŸŸæ·±åŒ–
```yaml
tasks:
  - diagnosis      # ç–¾ç—…è¯Šæ–­
  - treatment      # æ²»ç–—æ–¹æ¡ˆ
  - medication     # ç”¨è¯æŒ‡å¯¼
  - nutrition      # è¥å…»å»ºè®®
  - mental_health  # å¿ƒç†å¥åº·
```

#### æ–¹æ¡ˆ3ï¼šèƒ½åŠ›åˆ†è§£
```yaml
tasks:
  - reasoning      # æ¨ç†èƒ½åŠ›
  - summarization  # æ€»ç»“èƒ½åŠ›
  - translation    # ç¿»è¯‘èƒ½åŠ›
  - qa             # é—®ç­”èƒ½åŠ›
  - instruction    # æŒ‡ä»¤éµå¾ª
```

---

## è¿›é˜¶è¯é¢˜

### 1. Reptileä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ

Reptileæ˜¯MAMLçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œæ›´å®¹æ˜“å®ç°ï¼š

```python
# Reptileä¼ªä»£ç 
for epoch in range(num_epochs):
    for task in tasks:
        # å…‹éš†å½“å‰å‚æ•°
        old_params = clone(model.params)

        # åœ¨ä»»åŠ¡ä¸Šè®­ç»ƒKæ­¥
        for k in range(K):
            batch = sample_batch(task)
            loss = compute_loss(batch)
            optimizer.step()

        # å‘ä»»åŠ¡å‚æ•°æ–¹å‘ç§»åŠ¨
        model.params = old_params + epsilon * (model.params - old_params)
```

ä¼˜ç‚¹ï¼š
- å®ç°ç®€å•ï¼Œä¸éœ€è¦åŒå¾ªç¯
- å†…å­˜æ•ˆç‡é«˜
- æ€§èƒ½æ¥è¿‘MAML

### 2. ä¸LoRAç»“åˆ

åªå¯¹LoRAå‚æ•°åšMAMLï¼Œå›ºå®šbackboneï¼š

```python
# åªæ›´æ–°LoRAå‚æ•°
fast_weights = {
    name: param.clone()
    for name, param in model.named_parameters()
    if 'lora' in name
}
```

ä¼˜ç‚¹ï¼š
- æå¤§é™ä½å†…å­˜å’Œè®¡ç®—å¼€é”€
- é€‚é…é€Ÿåº¦æ›´å¿«
- ä¿æŒbase modelç¨³å®šæ€§

### 3. ä»»åŠ¡èšç±»

å°†ç›¸ä¼¼ä»»åŠ¡èšç±»ï¼Œæ¯ä¸ªclusterç‹¬ç«‹è®­ç»ƒï¼š

```python
clusters = {
    'medical': ['diagnosis', 'treatment', 'medication'],
    'technical': ['coding', 'debugging', 'documentation'],
}

# æ¯ä¸ªclusterç‹¬ç«‹è®­ç»ƒ
for cluster_name, cluster_tasks in clusters.items():
    train_maml(tasks=cluster_tasks, ...)
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: å†…å­˜ä¸è¶³

```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨FOMAML: `use_fomaml: true`
- å‡å°batch sizeå’Œmeta_batch_size
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨LoRAé€‚é…å™¨

### é—®é¢˜2: è®­ç»ƒä¸ç¨³å®š

```
Losså‡ºç°NaNæˆ–éœ‡è¡
```

**è§£å†³æ–¹æ¡ˆï¼š**
- é™ä½inner_lrå’Œouter_lr
- å¢åŠ æ¢¯åº¦è£å‰ª: `clip_grad: 0.5`
- æ£€æŸ¥æ•°æ®è´¨é‡
- ä½¿ç”¨warmup

### é—®é¢˜3: é€‚åº”æ•ˆæœå·®

```
Few-shotæ€§èƒ½ä¸å¦‚é¢„æœŸ
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ num_inner_steps
- æé«˜support setè´¨é‡
- æ£€æŸ¥ä»»åŠ¡ç›¸å…³æ€§
- å¢åŠ è®­ç»ƒæ­¥æ•°

---

## å‚è€ƒèµ„æº

### è®ºæ–‡
1. **MAMLåŸè®ºæ–‡**: "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" (Finn et al., ICML 2017)
2. **Reptile**: "On First-Order Meta-Learning Algorithms" (Nichol et al., 2018)
3. **LLM Meta-Learning**: "Language Models are Few-Shot Learners" (Brown et al., NeurIPS 2020)

### ä»£ç å‚è€ƒ
- verlæ¡†æ¶: https://github.com/volcengine/verl
- learn2learn: https://github.com/learnables/learn2learn (MAMLå®ç°å‚è€ƒ)
- higher: https://github.com/facebookresearch/higher (äºŒé˜¶ä¼˜åŒ–åº“)

---

## æ€»ç»“

### ä½•æ—¶ä½¿ç”¨MAML-SFTï¼Ÿ

**é€‚åˆåœºæ™¯ï¼š**
- éœ€è¦å¿«é€Ÿé€‚åº”å¤šä¸ªé¢†åŸŸ/ä»»åŠ¡
- æœ‰å¤šä¸ªç›¸å…³ä»»åŠ¡çš„æ•°æ®
- å¸Œæœ›æå‡few-shotå­¦ä¹ èƒ½åŠ›
- éœ€è¦ä¸ªæ€§åŒ–æ¨¡å‹

**ä¸é€‚åˆåœºæ™¯ï¼š**
- åªæœ‰å•ä¸€ä»»åŠ¡
- æ•°æ®é‡å……è¶³
- å¯¹è®­ç»ƒæ•ˆç‡è¦æ±‚æé«˜
- ä»»åŠ¡ä¹‹é—´å®Œå…¨æ— å…³

### å…³é”®è¦ç‚¹

1. **FOMAMLä¼˜å…ˆ**: å¯¹LLMæ¥è¯´ï¼ŒFOMAMLæ€§èƒ½æ¥è¿‘MAMLä½†æ•ˆç‡é«˜å¾—å¤š
2. **æ•°æ®è´¨é‡**: Support setçš„è´¨é‡æ¯”æ•°é‡æ›´é‡è¦
3. **è¶…å‚æ•°**: inner_lré€šå¸¸æ˜¯outer_lrçš„3-5å€
4. **ä»»åŠ¡é€‰æ‹©**: é€‰æ‹©ç›¸å…³ä½†ä¸é‡å çš„ä»»åŠ¡
5. **è¯„ä¼°**: å…³æ³¨few-shoté€‚åº”é€Ÿåº¦ï¼Œè€Œä¸åªæ˜¯æœ€ç»ˆæ€§èƒ½

ç¥å®éªŒé¡ºåˆ©ï¼ğŸš€
