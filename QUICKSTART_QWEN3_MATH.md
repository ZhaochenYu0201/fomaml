# Qwen3-4B + MATH å¿«é€Ÿå¼€å§‹æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ å¿«é€Ÿä½¿ç”¨Qwen3-4B-Instructæ¨¡å‹åœ¨MATHæ•°æ®é›†ä¸Šè¿›è¡ŒFOMAMLè®­ç»ƒã€‚

---

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: 4Ã—A100 (80GB) æˆ–åŒç­‰ç®—åŠ›
- **å†…å­˜**: 128GB+
- **å­˜å‚¨**: 100GB+

### è½¯ä»¶è¦æ±‚
```bash
# Pythonç‰ˆæœ¬
Python 3.8+

# æ ¸å¿ƒä¾èµ–
torch >= 2.0.0
transformers >= 4.35.0
datasets >= 2.14.0
pandas >= 2.0.0
numpy
tensordict
tqdm
wandb  # ç”¨äºæ—¥å¿—è®°å½•
omegaconf
```

### å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ç¯å¢ƒ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# verlæ¡†æ¶ï¼ˆå¦‚æœè¿˜æ²¡å®‰è£…ï¼‰
cd verl
pip install -e .

# å…¶ä»–ä¾èµ–
pip install datasets pandas transformers wandb omegaconf tensordict tqdm
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤0: å‡†å¤‡æ¨¡å‹

ç¡®ä¿ä½ å·²ç»ä¸‹è½½äº†Qwen3-4B-Instruct-2507æ¨¡å‹åˆ°æœ¬åœ°ï¼Œä¾‹å¦‚ï¼š
```
./models/Qwen3-4B-Instruct-2507/
```

### æ­¥éª¤1: å‡†å¤‡æ•°æ®

è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬ï¼š

```bash
python prepare_math_data.py \
    --output-dir ./data/math_meta \
    --support-ratio 0.30 \
    --query-ratio 0.40 \
    --validate
```

**å‚æ•°è¯´æ˜**ï¼š
- `--output-dir`: æ•°æ®è¾“å‡ºç›®å½•
- `--support-ratio`: Supporté›†æ¯”ä¾‹ï¼ˆç”¨äºå†…å¾ªç¯é€‚åº”ï¼‰
- `--query-ratio`: Queryé›†æ¯”ä¾‹ï¼ˆç”¨äºå…ƒæ¢¯åº¦è®¡ç®—ï¼‰
- `--validate`: éªŒè¯ç”Ÿæˆçš„æ•°æ®æ ¼å¼

**é¢„æœŸè¾“å‡º**ï¼š
```
æ‰¾åˆ° 7 ä¸ªæ•°å­¦é¢†åŸŸï¼š
  ğŸ“Œ Algebra: 1187 ä¸ªé—®é¢˜
  ğŸ“Œ Number Theory: 869 ä¸ªé—®é¢˜
  ğŸ“Œ Precalculus: 546 ä¸ªé—®é¢˜
  ğŸ“Œ Intermediate Algebra: 1207 ä¸ªé—®é¢˜
  ğŸ“Œ Counting and Probability: 474 ä¸ªé—®é¢˜
  ğŸ“Œ Geometry: 479 ä¸ªé—®é¢˜
  ğŸ“Œ Prealgebra: 871 ä¸ªé—®é¢˜

æ•°æ®é›†ç»Ÿè®¡:
                       task  support  query  test  total
                    algebra      356    475   356   1187
  counting_and_probability      142    189   143    474
                   geometry      143    191   145    479
       intermediate_algebra      362    482   363   1207
              number_theory      260    347   262    869
                precalculus      163    218   165    546
                 prealgebra      261    348   262    871
```

### æ­¥éª¤2: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config_qwen3_4b_math.yaml`ï¼Œç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼š

```yaml
model:
  partial_pretrain: "./models/Qwen3-4B-Instruct-2507"  # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
```

å¯é€‰ï¼šè°ƒæ•´è®­ç»ƒå‚æ•°
```yaml
meta:
  inner_lr: 1.0e-4          # å†…å¾ªç¯å­¦ä¹ ç‡
  num_inner_steps: 5        # å†…å¾ªç¯æ­¥æ•°
  outer_lr: 3.0e-5          # å¤–å¾ªç¯å­¦ä¹ ç‡
  meta_batch_size: 4        # æ¯æ¬¡å…ƒæ›´æ–°ä½¿ç”¨çš„ä»»åŠ¡æ•°

trainer:
  total_steps: 5000         # æ€»è®­ç»ƒæ­¥æ•°
  save_freq: 500            # checkpointä¿å­˜é¢‘ç‡
  test_freq: 100            # è¯„ä¼°é¢‘ç‡
```

### æ­¥éª¤3: å¼€å§‹è®­ç»ƒ

#### æ–¹å¼A: ä½¿ç”¨ä¸€é”®è„šæœ¬ï¼ˆæ¨èï¼‰

**Windows:**
```cmd
run_fomaml_qwen3_math.bat
```

**Linux/Mac:**
```bash
chmod +x run_fomaml_qwen3_math.sh
./run_fomaml_qwen3_math.sh
```

#### æ–¹å¼B: æ‰‹åŠ¨è¿è¡Œ

```bash
torchrun --nproc_per_node=4 \
    --master_port=29500 \
    maml_sft_trainer.py \
    --config-name config_qwen3_4b_math
```

**å‚æ•°è¯´æ˜**ï¼š
- `--nproc_per_node`: GPUæ•°é‡
- `--master_port`: åˆ†å¸ƒå¼è®­ç»ƒç«¯å£
- `--config-name`: é…ç½®æ–‡ä»¶åï¼ˆä¸å«.yamlåç¼€ï¼‰

---

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### Wandb Dashboard

è®­ç»ƒå¼€å§‹åï¼Œè®¿é—® [https://wandb.ai](https://wandb.ai) æŸ¥çœ‹å®æ—¶è®­ç»ƒæ—¥å¿—ã€‚

**å…³é”®æŒ‡æ ‡**ï¼š

1. **meta/loss**: å…ƒæŸå¤±ï¼ˆåº”æŒç»­ä¸‹é™ï¼‰
2. **meta/avg_adaptation_gap**: å¹³å‡é€‚åº”é—´éš™
   - å®šä¹‰: `query_loss - support_loss`
   - æ„ä¹‰: è¶Šå°è¯´æ˜æ¨¡å‹çš„å…ƒåˆå§‹åŒ–è¶Šå¥½
   - æœŸæœ›: éšè®­ç»ƒé€æ¸å‡å°

3. **meta/grad_norm**: æ¢¯åº¦èŒƒæ•°
   - åº”ä¿æŒç¨³å®šï¼Œä¸åº”çˆ†ç‚¸æˆ–æ¶ˆå¤±
   - å¦‚æœè¿‡å¤§(>10)ï¼Œè€ƒè™‘é™ä½å­¦ä¹ ç‡

4. **ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡**:
   - `{task_name}/support_loss`: æ¯ä¸ªä»»åŠ¡çš„supporté›†æŸå¤±
   - `{task_name}/query_loss`: æ¯ä¸ªä»»åŠ¡çš„queryé›†æŸå¤±
   - `{task_name}/adaptation_gap`: æ¯ä¸ªä»»åŠ¡çš„é€‚åº”é—´éš™

### è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
Step 100/5000:
  meta/loss: 2.45
  meta/avg_adaptation_gap: 0.32  â† æœŸæœ›è¿™ä¸ªå€¼ä¸‹é™
  meta/grad_norm: 1.2
  algebra/support_loss: 2.10
  algebra/query_loss: 2.42
  algebra/adaptation_gap: 0.32
  ...

Step 500/5000:
  meta/loss: 1.85
  meta/avg_adaptation_gap: 0.18  â† å·²ç»ä¸‹é™ï¼
  meta/grad_norm: 0.9
  ...
```

---

## ğŸ’¾ Checkpointç®¡ç†

### Checkpointç»“æ„

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜checkpointåˆ° `./checkpoints/fomaml_qwen3_4b_math/`ï¼š

```
checkpoints/fomaml_qwen3_4b_math/
â”œâ”€â”€ maml_checkpoint_step_500.pt
â”œâ”€â”€ maml_checkpoint_step_1000.pt
â”œâ”€â”€ maml_checkpoint_step_1500.pt
â””â”€â”€ ...
```

### åŠ è½½Checkpoint

```python
import torch

# åŠ è½½checkpoint
checkpoint = torch.load('checkpoints/fomaml_qwen3_4b_math/maml_checkpoint_step_5000.pt')

# æŸ¥çœ‹åŒ…å«çš„å†…å®¹
print(checkpoint.keys())
# dict_keys(['step', 'model_state_dict', 'optimizer_state_dict', 'config'])

# æ¢å¤è®­ç»ƒï¼ˆåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼‰
# trainer:
#   resume_from_path: "./checkpoints/fomaml_qwen3_4b_math/maml_checkpoint_step_2000.pt"
```

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ (CUDA out of memory)

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **å‡å°batch size**:
```yaml
meta:
  inner_batch_size: 2      # ä»4é™åˆ°2
  meta_batch_size: 2       # ä»4é™åˆ°2
  query_batch_size: 2
```

2. **å¼€å¯CPU offload**:
```yaml
model:
  fsdp_config:
    cpu_offload: true
```

3. **å‡å°‘GPUæ•°é‡ä½†å¢åŠ gradient accumulation**:
```bash
# ä½¿ç”¨2å¡è€Œä¸æ˜¯4å¡
torchrun --nproc_per_node=2 maml_sft_trainer.py ...
```

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**å½“å‰é¢„æœŸé€Ÿåº¦**: ~10-12 steps/å°æ—¶ (4Ã—A100)

**åŠ é€Ÿæ–¹æ³•**:

1. **å‡å°‘å†…å¾ªç¯æ­¥æ•°**:
```yaml
meta:
  num_inner_steps: 3  # ä»5é™åˆ°3
```

2. **å‡å°‘ä»»åŠ¡æ•°**:
   - åªé€‰æ‹©3-4ä¸ªæ ¸å¿ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒ

3. **ä½¿ç”¨æ··åˆç²¾åº¦**:
   - é…ç½®æ–‡ä»¶å·²é»˜è®¤ä½¿ç”¨bf16

### Q3: adaptation_gapä¸ä¸‹é™

å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°**:
```yaml
meta:
  inner_lr: 5.0e-5    # å°è¯•é™ä½
  outer_lr: 1.0e-5    # å°è¯•é™ä½
```

2. **å†…å¾ªç¯æ­¥æ•°ä¸å¤Ÿ**:
```yaml
meta:
  num_inner_steps: 10  # å¢åŠ åˆ°10
```

3. **ä»»åŠ¡ä¹‹é—´å·®å¼‚å¤ªå¤§**:
   - æ£€æŸ¥å„ä¸ªä»»åŠ¡çš„lossï¼Œå¦‚æœæŸäº›ä»»åŠ¡ç‰¹åˆ«é«˜ï¼Œè€ƒè™‘ç§»é™¤

### Q4: æ•°æ®å‡†å¤‡å¤±è´¥

```bash
# é”™è¯¯: æ— æ³•ä¸‹è½½MATHæ•°æ®é›†

# è§£å†³æ–¹æ¡ˆ1: ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
python prepare_math_data.py ...

# è§£å†³æ–¹æ¡ˆ2: æ‰‹åŠ¨ä¸‹è½½
# 1. è®¿é—® https://github.com/hendrycks/math
# 2. ä¸‹è½½æ•°æ®é›†
# 3. ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### è®­ç»ƒæ—¶é•¿
- **æ€»æ­¥æ•°**: 5000 steps
- **é€Ÿåº¦**: ~10 steps/å°æ—¶ (4Ã—A100)
- **æ€»æ—¶é•¿**: ~40-50å°æ—¶

### æ€§èƒ½æŒ‡æ ‡

æˆåŠŸçš„è®­ç»ƒåº”è¯¥æ˜¾ç¤ºï¼š

```
åˆå§‹é˜¶æ®µ (0-500 steps):
  meta/loss: 3.0 â†’ 2.2
  meta/avg_adaptation_gap: 0.8 â†’ 0.4

ä¸­æœŸ (500-2500 steps):
  meta/loss: 2.2 â†’ 1.5
  meta/avg_adaptation_gap: 0.4 â†’ 0.2

åæœŸ (2500-5000 steps):
  meta/loss: 1.5 â†’ 1.0
  meta/avg_adaptation_gap: 0.2 â†’ 0.1
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- âœ… `meta/loss` æŒç»­ä¸‹é™
- âœ… `adaptation_gap` é€æ¸å‡å°ï¼ˆè¯´æ˜å…ƒåˆå§‹åŒ–è¶Šæ¥è¶Šå¥½ï¼‰
- âœ… å„ä»»åŠ¡çš„lossç›¸å¯¹å¹³è¡¡

---

## ğŸ¯ ä¸‹ä¸€æ­¥

è®­ç»ƒå®Œæˆåï¼š

1. **Few-shotè¯„ä¼°**:
```bash
python evaluate_few_shot.py \
    --model-path ./checkpoints/fomaml_qwen3_4b_math/maml_checkpoint_step_5000.pt \
    --n-shots 0 5 10 25 50
```

2. **å¯¹æ¯”å®éªŒ**:
   - è®­ç»ƒbaseline SFTæ¨¡å‹ï¼ˆä¸ä½¿ç”¨FOMAMLï¼‰
   - å¯¹æ¯”few-shotæ€§èƒ½

3. **åˆ†æç»“æœ**:
   - æŸ¥çœ‹wandbæ—¥å¿—
   - ç»˜åˆ¶å­¦ä¹ æ›²çº¿
   - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [FOMAMLå®ç°è¯¦è§£](FOMAML_IMPLEMENTATION_DETAILS.md)
- [æ•°æ®å‡†å¤‡å®Œæ•´æŒ‡å—](DATA_PREPARATION_GUIDE.md)
- [å®éªŒè®¾è®¡æ–¹æ¡ˆ](EXPERIMENT_DESIGN_MATH_SCIENCE.md)

---

## ğŸ“§ é—®é¢˜åé¦ˆ

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æ£€æŸ¥æ—¥å¿—è¾“å‡º
2. æŸ¥çœ‹wandb dashboard
3. å‚è€ƒå¸¸è§é—®é¢˜éƒ¨åˆ†
4. åœ¨é¡¹ç›®ä¸­æissue

---

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€
